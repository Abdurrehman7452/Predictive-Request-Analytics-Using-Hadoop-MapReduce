# Predictive Request Analytics Using Hadoop MapReduce

This project simulates a client-server infrastructure that processes API requests, predicts their success/failure, and evaluates the accuracy and cost implications of those predictions using **Hadoop MapReduce**.

## 📌 Overview

The entire pipeline consists of **three MapReduce jobs** that work in sequence to:

1. **Simulate requests and predictions**
2. **Evaluate prediction accuracy**
3. **Generate per-client performance analytics**

This is ideal for studying distributed processing, predictive modeling, and analyzing infrastructure efficiency under load.

---

## 🧠 Problem Context

In a real-world microservices setup, every client API request may succeed or fail depending on server availability. Predictions of outcomes can be generated by a model or heuristic. We want to analyze:

- **Prediction accuracy**
- **API costs for successful calls**
- **Client-level performance summaries**

---

## 🗂️ Project Structure
```text
predictive-request-analytics-mapreduce/
├── mapper1.py # Parses raw logs into structured request and prediction records
├── reducer1.py # Matches predictions with actual results, simulates server logic
├── mapper2.py # Restructures enriched data from reducer1
├── reducer2.py # Evaluates prediction correctness & maps cost
├── mapper3.py # Groups data by client
├── reducer3.py # Aggregates final per-client analytics
├── small_data.txt # Sample dataset (Requests + Predictions)
├── README.md # You're reading it
```

---

## 🚀 How It Works

### 🧩 MapReduce Job 1: Request Matching & Status Simulation
- Simulates server load & availability per API endpoint
- Assigns actual status codes (200/500) to requests
- Joins predictions with actual outcomes

### 🧮 MapReduce Job 2: Prediction Accuracy & Cost Evaluation
- Calculates if each prediction was correct
- Maps cost of each API request
- Outputs enriched records with flags

### 📊 MapReduce Job 3: Per-Client Aggregation
- Tallies total requests, correct predictions, and cost
- Produces final summary:  
  `client_id correct_predictions/total_requests total_cost`

---

## 🧪 Sample Input

### `small_data.txt`
```plaintext
r001 c01 user/profile 00:00:00 2.0
r002 c02 user/profile 00:00:00 2.0
r003 c03 user/settings 00:01:02
...
r010 c04 user/profile 00:08:05 3.0

# Predictions
r001 500
r002 500
r003 200
...
r010 200
```

## 📤 Running the Pipeline

Each stage can be run using Hadoop Streaming:
```
# Job 1
hadoop jar /path/to/hadoop-streaming.jar \
  -input input_dir \
  -output output1 \
  -mapper mapper1.py \
  -reducer reducer1.py

# Job 2
hadoop jar /path/to/hadoop-streaming.jar \
  -input output1 \
  -output output2 \
  -mapper mapper2.py \
  -reducer reducer2.py

# Job 3
hadoop jar /path/to/hadoop-streaming.jar \
  -input output2 \
  -output final_output \
  -mapper mapper3.py \
  -reducer reducer3.py
```
Make sure all scripts are executable:
```
chmod +x mapper*.py reducer*.py
```
---

## ✅ Final Output Format

Example:
```
c01 2/3 400
c02 1/2 100
```
---

## 🧰 Tech Stack

- Hadoop Streaming
- Python 3.x
- Shell CLI for orchestration
- Unix-based OS (preferred for Hadoop compatibility)

---

# 🧑‍💻 Author

**Abdur Rehman**  
_Bachelor's in Data Science_  
Passionate about simple but scalable code for big data pipelines.
